import json
import os
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_classic.chains import RetrievalQA
from dotenv import load_dotenv

# Load environment variables (API Keys) from .env file
load_dotenv()

# The source file generated by ingest.py
DOC_FILE = "documents.json"

def load_documents():
    """
    Reads the processed JSON file and converts data back into 
    LangChain Document objects for the vector store.
    """
    if not os.path.exists(DOC_FILE):
        return []
    
    with open(DOC_FILE, "r", encoding="utf-8") as f:
        raw_docs = json.load(f)
    
    # Reconstruct Document objects with content and their original metadata (source, page)
    return [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in raw_docs]

def build_qa_chain():
    """
    Initializes the RAG (Retrieval-Augmented Generation) pipeline:
    1. Loads documents
    2. Creates vector embeddings
    3. Sets up the FAISS vector store
    4. Configures the LLM and Retrieval chain
    """
    documents = load_documents()
    if not documents:
        return None
    
    # Initialize OpenAI embeddings model
    embeddings = OpenAIEmbeddings()
    
    # Create a searchable vector database in memory using FAISS
    vectorstore = FAISS.from_documents(documents, embeddings)
    
    # Initialize the LLM (GPT-4o-mini) with 0 temperature for factual consistency
    llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")
    
    # Create the RetrievalQA chain
    # k=3 retrieves the 3 most relevant document chunks for context
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    return qa_chain



def role_based_query(qa_chain, query, role="Patient"):
    """
    Wraps the user query with role-based instructions before sending to the LLM.
    This acts as a security layer to control data visibility.
    """
    if qa_chain is None:
        return {
            "result": "No documents found. Admin needs to upload files first.", 
            "source_documents": []
        }
    
    # Augment the prompt with identity-based rules
    prompt = f"""You are a hospital assistant. User role: {role}
    Rules:
    - If Patient: do NOT reveal private medical records of others. Only provide general guidance or specific personal info if available.
    - If Staff/Admin: provide detailed professional access to records and policies.
    - Always cite sources if available.
    
    Question: {query}"""
    
    # Execute the chain and return the answer + source metadata
    return qa_chain.invoke(prompt)